
'''
obs : ì ˆëŒ€ì¢Œí‘œë¡œ ë¯¼ë§¥ìŠ¤ìŠ¤ì¼€ì¼ë§(4000,4000,400)
learn ì‹œì  : 3ê°œì˜ ë°°ì¹˜ë¥¼ ëª¨ìœ¼ê³ , ìµœì†Œ íŠ¸ëœì§€ì…˜ ìˆ˜ ê±¸ì–´ë†“ìŒ

'''
import numpy as np
import torch
from easydict import EasyDict
from ding.model import VAC
from ding.policy import PPOPolicy
from ding.utils import set_pkg_seed
from ding.torch_utils import to_device, unsqueeze
from tensorboardX import SummaryWriter
from socket_env import SocketEnv
from ding.envs import BaseEnvTimestep

# ===== ì„¤ì • =====
TOTAL_STEPS = 50000
BATCH_SIZE = 64
EPOCH_PER_COLLECT = 8
UNROLL_LEN = 1

# ===== í™˜ê²½ ì´ˆê¸°í™” =====
env = SocketEnv({
    'map_size': 4000,
    'max_step': 1000,
    'win_reward': 5.0,
    'num_detectable': 2
})
OBS_SHAPE = env.observation_space.shape[0]
print(OBS_SHAPE)
ACTION_DIM = env.action_space.n

config = dict(
    type='ppo',
    cuda=False,
    multi_gpu=False,
    on_policy=True,
    priority=False,
    priority_IS_weight=False,
    recompute_adv=True,
    action_space='discrete',
    nstep_return=False,
    multi_agent=False,
    transition_with_policy_data=True,

    model=dict(
        obs_shape=OBS_SHAPE,
        action_shape=ACTION_DIM,
        encoder_hidden_size_list=[64, 64],
        actor_head_hidden_size=64,
        critic_head_hidden_size=64,
        share_encoder=True,
        action_space='discrete',
    ),

    learn=dict(
        epoch_per_collect=8, #ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµí• ì§€, ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì¬ì‚¬ìš©í• ì§€
        batch_size=32,
        learning_rate=3e-4,
        lr_scheduler=None,
        value_weight=0.5,
        entropy_weight=0.01, # íƒí—˜ì •ë„
        clip_ratio=0.15, # ë†’ì„ìˆ˜ë¡ ì •ì±…ì´ ê¸‰ë³€, ê¸°ì¡´ì •ì±…ì—ì„œ ëª‡í¼ì„¼íŠ¸ ë³€í™”ë¥¼ í—ˆìš©í•˜ëŠ”ì§€, 0.10 -> 10% ë³€í™” í—ˆìš© ê·¸ ì´ìƒì€ X
        adv_norm=True, #false or true / ì •ê·œí™” ì—¬ë¶€ ì¤‘ìš”
        value_norm=False, #false or true / ì •ê·œí™” ì—¬ë¶€ ì¤‘ìš” / ì´ê±° ì¼œë©´ nanìœ¼ë¡œ ê½‰ì°¨ëŠ” ì˜¤ë¥˜ë°œìƒìƒ
        ppo_param_init=True,
        grad_clip_type='clip_norm',
        grad_clip_value=1,
        ignore_done=False,
    ),

    collect=dict(
        unroll_len=1,
        discount_factor=0.99,
        gae_lambda=0.95,
    ),

    eval=dict(
        evaluator=dict(
            eval_freq=1000,
            n_episode=5,
            render=False,
        )
    ),

    other=dict(
        eps=dict( 
            type='exp',
            start=1.0,
            end=0.05,
            decay=1000,
        )
    ),
)

cfg = EasyDict(config)

# ===== ì´ˆê¸°í™” =====
set_pkg_seed(0, use_cuda=cfg.cuda)
obs = env.reset()
model = VAC(**cfg.model)
print(model.actor)
device = 'cuda' if cfg.cuda else 'cpu'
model.to(device)
policy = PPOPolicy(cfg, model=model)
writer = SummaryWriter('./tensorlog_ppo')
first_obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)
print("first obs :" , first_obs)

#torch.autograd.set_detect_anomaly(True) 
#ì˜¤ë¥˜ ì°¾ëŠ” ì½”ë“œ?

# ===== PPO ìˆ˜ì§‘ ë° í•™ìŠµ =====
global_step = 0
episode_idx = 0
episode_reward = 0
episode_buffer = []  # ì—¬ëŸ¬ ì—í”¼ì†Œë“œ ì €ì¥
episode_batch_size = 3  # ëª‡ ê°œì˜ ì—í”¼ì†Œë“œ ëª¨ì•„ì„œ í•™ìŠµí• ì§€
min_transitions = 0 # ì´ì•Œë°ë¯¸ì§€ ê³ ë ¤í•´ì„œ ìµœì íšŸìˆ˜ì¡°ì •
all_episode_rewards = []  
probs_history = []  # í–‰ë™ í™•ë¥  ì €ì¥
learn_steps = []    # í•™ìŠµ ì‹œì  ì €ì¥

while global_step < TOTAL_STEPS:
    trajectory = []
    step = 0
    done = False
    obs_tensor = torch.zeros_like(first_obs)

    while not done and step < env.max_step:
        #obs_tensor = torch.rand(1, OBS_SHAPE)
        #print("obs :", obs_tensor)
        with torch.no_grad():
            output = model(obs_tensor, mode='compute_actor_critic')
            print("Actor logits:", output['logit'])
            print("Value:", output['value'])
            collect_output = policy.collect_mode.forward({0: obs_tensor})[0]      
            print("selected action:", collect_output['action'])

        logits = collect_output['logit']
        logits = torch.clamp(logits, -10, 10)  # ê³¼ë„í•œ ìŒìˆ˜/ì–‘ìˆ˜ ë°©ì§€
        probs = torch.nn.functional.softmax(logits, dim=-1)
        probs_history.append(probs.squeeze(0).cpu().numpy().tolist())

        print(f"[DEBUG] logits: {logits} â†’ probs: {probs}")

        action = collect_output['action'].item()
        #action = 1 #action = 2 (ë’¤ë¡œê°€ê¸°) ë¡œ ê³ ì •
        env_action = action + 1
        timestep = env.step(np.array([env_action], dtype=np.int64))

        if np.isnan(timestep.obs).any() or np.isinf(timestep.obs).any():
            print("[ERROR] Invalid obs detected:", timestep.obs)
        if np.isnan(timestep.reward).any() or np.isinf(timestep.reward).any():
            print("[ERROR] Invalid reward detected:", timestep.reward)

        # ìœ„ì¹˜ì •ë³´ì˜ ë‹¤ìŒ ê´€ì¸¡ê°’ì„ tensorë¡œ ë³€í™˜í•˜ê³  ë³€í™”ëŸ‰ ê³„ì‚°
        max_vals = torch.tensor([4000.0, 4000.0, 400.0] * (OBS_SHAPE // 3), device=device)
        obs_tensor_next = timestep.obs / max_vals  # ì ˆëŒ€ì¢Œí‘œ â†’ [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
        obs_tensor_next_unsqueezed = obs_tensor_next.unsqueeze(0)

        # ë³´ìƒ ë° ì¢…ë£Œ ì—¬ë¶€ ì²˜ë¦¬
        scaled_reward = timestep.reward[0] * 0.1  # ë³´ìƒ ìŠ¤ì¼€ì¼ë§
        reward_tensor = torch.tensor(scaled_reward, dtype=torch.float32)
        done_tensor = torch.tensor(timestep.done, dtype=torch.float32)

        # í™˜ê²½ ê²°ê³¼ë¥¼ timestep í…ì„œë¡œ ë³€í™˜
        timestep_tensor = BaseEnvTimestep(
            obs=obs_tensor_next,
            reward=reward_tensor,
            done=done_tensor,
            info=timestep.info
        )

        # transition ì²˜ë¦¬
        transition = {
            'obs': obs_tensor.squeeze(0),
            'next_obs': obs_tensor_next.squeeze(0),
            'action': collect_output['action'].squeeze(0),
            'logit': collect_output['logit'].squeeze(0),
            'value': collect_output['value'].squeeze(0),
            'reward': reward_tensor,  # ë°˜ë“œì‹œ float tensor
            'done': done_tensor,      # ë°˜ë“œì‹œ float tensor
        }
        trajectory.append(transition)

        trajectory.append(transition)

        print(f"[STEP {global_step}] EP {episode_idx} | Action: {env_action} | Reward: {reward_tensor.item():.2f}")

        # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™
        obs = timestep.obs
        obs_tensor = obs_tensor_next_unsqueezed  # ë³€í™”ëŸ‰ ê¸°ì¤€ obs ìœ ì§€
        episode_reward += reward_tensor.item()
        global_step += 1
        step += 1
        done = timestep.done

    print(f"[EP {episode_idx}] Reward: {episode_reward:.2f}, Steps: {step}")
    writer.add_scalar("episode_reward", episode_reward, global_step)

    all_episode_rewards.append(episode_reward)

    episode_buffer.append(trajectory)
    episode_idx += 1
    obs = env.reset()
    first_obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)  # ğŸ”¥ ì¤‘ìš”: ìƒˆë¡œìš´ ì—í”¼ì†Œë“œì˜ ê¸°ì¤€ obs
    obs_tensor = torch.zeros_like(first_obs)  # ë³€í™”ëŸ‰ì€ ì²˜ìŒì—” 0
    episode_reward = 0

#learnë˜ëŠ” ë¶€ë¶„ë¶„

    if len(episode_buffer) >= episode_batch_size:
        combined_transitions = sum(episode_buffer, [])
        if len(combined_transitions) >= min_transitions:
            train_data = policy._get_train_sample(combined_transitions)

            for t in train_data:
                for key in t:
                    if isinstance(t[key], (bool, np.bool_)):
                        t[key] = torch.tensor(float(t[key]), dtype=torch.float32)

            if len(train_data) > 1:
                print(f">>> [POLICY LEARN] step {global_step} | {len(combined_transitions)} transitions")
                learn_output = policy._forward_learn(train_data)
                learn_steps.append(global_step)
                for name, param in policy._learn_model.named_parameters():
                    if param.grad is not None:
                        print(f"[GRAD] {name} grad norm: {param.grad.norm().item():.6f}")

                print("=== Learn Output ===")
                print(learn_output)
                for stat in learn_output:
                    for k, v in stat.items():
                        writer.add_scalar(k, v, global_step)
        episode_buffer.clear()

# ì¢…ë£Œ ì‹œ í™˜ê²½ ë‹«ê¸°
env.close()

#ëª¨ë¸ ì €ì¥
save_path = "C:/Users/CIL2/Desktop/DI-engine-main/unreal/learned_models/ppo_model.pth"
torch.save(policy._learn_model.state_dict(), save_path)
print(f"[MODEL SAVED] to {save_path}")

# ===== ë³´ìƒ ê·¸ë˜í”„ =====
import matplotlib.pyplot as plt

episode_rewards = all_episode_rewards 
avg_rewards = []
window_size = 10
for i in range(len(episode_rewards)):
    start = max(0, i - window_size + 1)
    avg_rewards.append(np.mean(episode_rewards[start:i+1]))

plt.figure(figsize=(10, 5))
plt.plot(episode_rewards, label='Episode Reward', alpha=0.5, color='blue')
plt.plot(avg_rewards, label='10-Episode Moving Avg', linewidth=2, color='orange')
plt.title("Episode Rewards over Time (PPO)")
plt.xlabel("Episode")
plt.ylabel("Reward")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# ===== í–‰ë™ í™•ë¥  ê·¸ë˜í”„ =====
probs_array = np.array(probs_history)

plt.figure(figsize=(12, 6))
for i in range(probs_array.shape[1]):
    plt.plot(probs_array[:, i], label=f'Action {i}')

for step in learn_steps:
    plt.axvline(x=step, color='gray', linestyle='--', alpha=0.6, label='Learn Step' if step == learn_steps[0] else "")

plt.title("Action Probability over Time")
plt.xlabel("Step")
plt.ylabel("Probability")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
